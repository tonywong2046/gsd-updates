#!/usr/bin/env python3
"""
Sociology Journal Fetcher â€” CrossRef API Edition
- å›½é™…æœŸåˆŠï¼šCrossRef APIï¼ˆæŒ‰ ISSN + æ—¥æœŸæŸ¥è¯¢ï¼Œæ— éœ€ RSS URLï¼‰
- ä¸­æ–‡æœŸåˆŠï¼šä¿ç•™ CNKI RSS
- è¿‡æ»¤ä¹¦è¯„ â†’ Gemini/Groq è¯„åˆ† â†’ å†™å…¥ Google Sheets
"""

import subprocess, json, os, re, xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.request import urlopen, Request

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SHEET_ID    = "1MCcEqV2OGkxFofWSRI6BW2OFYG35cNDHC2olbm43NWc"
SHEET_RANGE = "è®ºæ–‡"
MAILTO      = "wangsenhu@gmail.com"   # CrossRef polite pool
GEMINI_KEYS = [k for k in [
    os.environ.get("GEMINI_API_KEY", ""),
    os.environ.get("GEMINI_API_KEY_2", ""),
    os.environ.get("GEMINI_API_KEY_3", ""),
] if k]
GROQ_API_KEY       = os.environ.get("GROQ_API_KEY", "")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")
SGT = timezone(timedelta(hours=8))  # æ–°åŠ å¡æ—¶é—´ UTC+8
TARGET_DATE = (datetime.now(SGT) - timedelta(days=1)).strftime("%Y-%m-%d")

# â”€â”€ å›½é™…æœŸåˆŠï¼ˆCrossRefï¼ŒæŒ‰ ISSNï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JOURNALS = [
    # ç»¼åˆç¤¾ä¼šå­¦
    ("American Sociological Review",      "ç»¼åˆç¤¾ä¼šå­¦",    "0003-1224"),
    ("American Journal of Sociology",     "ç»¼åˆç¤¾ä¼šå­¦",    "0002-9602"),
    ("Annual Review of Sociology",        "ç»¼åˆç¤¾ä¼šå­¦",    "0360-0572"),
    ("Social Forces",                     "ç»¼åˆç¤¾ä¼šå­¦",    "0037-7732"),
    ("Sociological Methods & Research",   "ç»¼åˆç¤¾ä¼šå­¦",    "0049-1241"),
    ("European Sociological Review",      "ç»¼åˆç¤¾ä¼šå­¦",    "0266-7215"),
    ("British Journal of Sociology",      "ç»¼åˆç¤¾ä¼šå­¦",    "0007-1315"),
    ("Sociology (BSA)",                   "ç»¼åˆç¤¾ä¼šå­¦",    "0038-0385"),
    ("Work, Employment and Society",      "ç»¼åˆç¤¾ä¼šå­¦",    "0950-0170"),
    ("Chinese Sociological Review",       "ç»¼åˆç¤¾ä¼šå­¦",    "2162-0555"),
    # ç§»æ°‘ä¸æ—è£”
    ("International Migration Review",           "ç§»æ°‘ä¸æ—è£”", "0197-9183"),
    ("Journal of Ethnic and Migration Studies",  "ç§»æ°‘ä¸æ—è£”", "1369-183X"),
    ("International Migration",                  "ç§»æ°‘ä¸æ—è£”", "0020-7985"),
    # è®¡ç®—ç¤¾ä¼šç§‘å­¦
    ("Journal of Computational Social Science", "è®¡ç®—ç¤¾ä¼šç§‘å­¦", "2432-2717"),
    ("Social Science Computer Review",          "è®¡ç®—ç¤¾ä¼šç§‘å­¦", "0894-4393"),
    ("Nature Human Behaviour",                  "è®¡ç®—ç¤¾ä¼šç§‘å­¦", "2397-3374"),
    # ç¤¾ä¼šç½‘ç»œ
    ("Social Networks",  "ç¤¾ä¼šç½‘ç»œ", "0378-8733"),
    ("Network Science",  "ç¤¾ä¼šç½‘ç»œ", "2050-1242"),
    # ç¤¾ä¼šåˆ†å±‚ä¸æµåŠ¨
    ("Research in Social Stratification and Mobility", "ç¤¾ä¼šåˆ†å±‚ä¸æµåŠ¨", "0276-5624"),
    ("Social Science Research",                        "ç¤¾ä¼šåˆ†å±‚ä¸æµåŠ¨", "0049-089X"),
    # åŒ»å­¦ç¤¾ä¼šå­¦
    ("Social Science & Medicine",           "åŒ»å­¦ç¤¾ä¼šå­¦", "0277-9536"),
    ("Journal of Health and Social Behavior","åŒ»å­¦ç¤¾ä¼šå­¦", "0022-1465"),
    ("Sociology of Health & Illness",       "åŒ»å­¦ç¤¾ä¼šå­¦", "0141-9889"),
    # è€å¹´å­¦
    ("Journals of Gerontology Series B", "è€å¹´å­¦", "1079-5014"),
    ("The Gerontologist",                "è€å¹´å­¦", "0016-9013"),
    ("Journal of Aging and Health",      "è€å¹´å­¦", "0898-2643"),
    # å©šå§»ä¸å®¶åº­
    ("Journal of Marriage and Family", "å©šå§»ä¸å®¶åº­", "0022-2445"),
    ("Journal of Family Issues",       "å©šå§»ä¸å®¶åº­", "0192-513X"),
    # äººå£å­¦
    ("Demography",                      "äººå£å­¦", "0070-3370"),
    ("Population and Development Review","äººå£å­¦", "0098-7921"),
    ("Population Studies",              "äººå£å­¦", "0032-4728"),
    ("European Journal of Population",  "äººå£å­¦", "0168-6577"),
    ("Demographic Research",            "äººå£å­¦", "1435-9871"),
]

# â”€â”€ ä¸­æ–‡æœŸåˆŠï¼ˆCNKI RSSï¼ŒCrossRef æœªæ”¶å½•ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHINESE_JOURNALS = [
    ("ç¤¾ä¼šå­¦ç ”ç©¶",   "ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠ", "https://rss.cnki.net/knavi/rss/SHXJ?pcode=CJFD"),
    ("äººå£ç ”ç©¶",     "ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠ", "https://rss.cnki.net/knavi/rss/RKYZ?pcode=CJFD"),
    ("ä¸­å›½ç¤¾ä¼šç§‘å­¦", "ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠ", "https://rss.cnki.net/knavi/rss/ZSHK?pcode=CJFD"),
    ("ç¤¾ä¼šå­¦è¯„è®º",   "ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠ", "https://rss.cnki.net/knavi/rss/SHXP?pcode=CJFD"),
    ("ä¸­å›½äººå£ç§‘å­¦", "ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠ", "https://rss.cnki.net/knavi/rss/ZKRK?pcode=CJFD"),
]

# â”€â”€ ä¹¦è¯„è¿‡æ»¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BOOK_REVIEW_KEYWORDS = [
    "book review", "review of ", "reviews of ",
    "book notice", "book symposium", "review essay", "book forum",
]

def is_book_review(title):
    t = title.lower()
    if any(kw in t for kw in BOOK_REVIEW_KEYWORDS):
        return True
    if re.search(r'\bISBN\b', title, re.IGNORECASE):
        return True
    if re.search(r'\bpp\.', title) and re.search(r'[Â£$â‚¬]\d', title):
        return True
    if re.search(r'\. By [A-Z].+?\.\s+\w+:', title):
        return True
    return False

# â”€â”€ CrossRef æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_crossref(journal_name, field, issn):
    try:
        url = (
            f"https://api.crossref.org/works"
            f"?filter=issn:{issn},from-pub-date:{TARGET_DATE},until-pub-date:{TARGET_DATE}"
            f"&rows=50&select=title,author,DOI,URL,published,published-online,type"
            f"&mailto={MAILTO}"
        )
        req = Request(url, headers={"User-Agent": f"SociologyBot/1.0 (mailto:{MAILTO})"})
        with urlopen(req, timeout=20) as resp:
            data = json.loads(resp.read())

        items = data.get("message", {}).get("items", [])
        articles = []
        for item in items:
            if item.get("type") != "journal-article":
                continue

            title_list = item.get("title", [])
            title = title_list[0].strip() if title_list else ""
            if not title or is_book_review(title):
                continue

            # æ—¥æœŸï¼šä¼˜å…ˆ published-online
            pub = item.get("published-online") or item.get("published") or {}
            parts = pub.get("date-parts", [[]])[0]
            if len(parts) >= 3:
                article_date = f"{parts[0]:04d}-{parts[1]:02d}-{parts[2]:02d}"
            else:
                continue  # æ—¥æœŸä¸å®Œæ•´è·³è¿‡

            if article_date != TARGET_DATE:
                continue

            # ä½œè€…
            authors = []
            for a in item.get("author", []):
                name = f"{a.get('given','')} {a.get('family','')}".strip()
                if name:
                    authors.append(name)

            doi  = item.get("DOI", "")
            link = item.get("URL") or (f"https://doi.org/{doi}" if doi else "")

            articles.append({
                "journal": journal_name, "field": field,
                "title":   title,
                "authors": ", ".join(authors) or "N/A",
                "date":    article_date,
                "link":    link,
            })

        print(f"  âœ… {journal_name}: {len(articles)} ç¯‡")
        return articles
    except Exception as e:
        print(f"  âš ï¸  {journal_name}: å¤±è´¥ ({e})")
        return []

# â”€â”€ RSS æŠ“å–ï¼ˆä¸­æ–‡æœŸåˆŠï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_date(date_str):
    if not date_str:
        return ""
    import email.utils
    try:
        parsed = email.utils.parsedate_to_datetime(date_str)
        return parsed.astimezone(timezone.utc).strftime("%Y-%m-%d")
    except:
        pass
    try:
        cleaned = date_str.strip().replace("Z", "+00:00")
        parsed = datetime.fromisoformat(cleaned)
        return parsed.astimezone(timezone.utc).strftime("%Y-%m-%d")
    except:
        pass
    if len(date_str) >= 10 and date_str[4] == "-" and date_str[7] == "-":
        return date_str[:10]
    return ""

def fetch_rss(journal_name, field, url):
    try:
        req = Request(url, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "application/rss+xml, application/xml, text/xml, */*",
        })
        with urlopen(req, timeout=15) as resp:
            content = resp.read()
        content_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '',
                             content.decode("utf-8", errors="replace"))
        root = ET.fromstring(content_str.encode("utf-8"))
        ns = {"dc": "http://purl.org/dc/elements/1.1/",
              "prism": "http://prismstandard.org/namespaces/basic/2.0/"}
        articles = []
        for item in root.iter("item"):
            title_el = item.find("title")
            title = title_el.text.strip() if title_el is not None and title_el.text else ""
            if is_book_review(title):
                continue
            link_el = item.find("link")
            link = link_el.text.strip() if link_el is not None and link_el.text else ""
            authors = [c.text.strip() for c in item.findall("dc:creator", ns) if c.text]
            pub_date = ""
            for tag in ["pubDate", "dc:date", "prism:coverDate"]:
                el = item.find(tag, ns) if ":" in tag else item.find(tag)
                if el is not None and el.text:
                    pub_date = el.text.strip()
                    break
            if normalize_date(pub_date) == TARGET_DATE:
                articles.append({
                    "journal": journal_name, "field": field, "title": title,
                    "authors": ", ".join(authors) or "N/A",
                    "date": normalize_date(pub_date), "link": link,
                })
        print(f"  âœ… {journal_name}: {len(articles)} ç¯‡")
        return articles
    except Exception as e:
        print(f"  âš ï¸  {journal_name}: å¤±è´¥ ({e})")
        return []

# â”€â”€ è¯„åˆ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def score_articles(articles):
    if not articles:
        return articles
    import time

    titles_list = "\n".join([
        f"{i+1}. [{a['journal']}] {a['title']}" for i, a in enumerate(articles)
    ])
    prompt = f"""ä½ æ˜¯ç¤¾ä¼šå­¦é¢†åŸŸçš„ä¸“å®¶æ•™æˆã€‚è¯·å¯¹ä»¥ä¸‹å­¦æœ¯è®ºæ–‡é€ä¸€ç»™å‡ºæ¨èæŒ‡æ•°ï¼ˆ1-5æ˜Ÿï¼‰å’Œä¸€å¥è¯ç†ç”±ã€‚

è¯„åˆ†æ ‡å‡†ï¼š
â˜…â˜…â˜…â˜…â˜… é‡å¤§ç†è®ºçªç ´æˆ–æ–¹æ³•åˆ›æ–°ï¼Œé¢†åŸŸé‡Œç¨‹ç¢‘
â˜…â˜…â˜…â˜…â˜† æœ‰é‡è¦ç†è®ºæˆ–å®è¯è´¡çŒ®ï¼Œå€¼å¾—ç²¾è¯»
â˜…â˜…â˜…â˜†â˜† æ‰å®ç ”ç©¶ï¼Œæœ‰ä¸€å®šå‚è€ƒä»·å€¼
â˜…â˜…â˜†â˜†â˜† è¾ƒä¸ºå¸¸è§„ï¼Œé€‰è¯»
â˜…â˜†â˜†â˜†â˜† è´¡çŒ®æœ‰é™

è®ºæ–‡åˆ—è¡¨ï¼š
{titles_list}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
[
  {{"index": 1, "score": "â˜…â˜…â˜…â˜…â˜† ä¸€å¥è¯ç†ç”±"}},
  {{"index": 2, "score": "â˜…â˜…â˜…â˜†â˜† ä¸€å¥è¯ç†ç”±"}}
]"""

    def parse_scores(content):
        content = content.strip()
        if content.startswith("```"):
            content = content.split("\n", 1)[-1].rsplit("```", 1)[0]
        start, end = content.find("["), content.rfind("]") + 1
        if start == -1 or end == 0:
            raise ValueError(f"No JSON array: {content[:80]!r}")
        return json.loads(content[start:end])

    def apply_scores(scores):
        score_map = {s["index"]: s["score"] for s in scores}
        for i, a in enumerate(articles):
            a["score"] = score_map.get(i + 1, "â˜…â˜…â˜…â˜†â˜† æš‚æ— è¯„åˆ†")

    # 1. Gemini
    def call_gemini(api_key):
        payload = json.dumps({
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"maxOutputTokens": 2000, "thinkingConfig": {"thinkingBudget": 0}},
        }).encode()
        req = Request(
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}",
            data=payload, headers={"Content-Type": "application/json"},
        )
        with urlopen(req, timeout=60) as resp:
            result = json.loads(resp.read())
        parts = result["candidates"][0]["content"]["parts"]
        text = next((p["text"] for p in reversed(parts) if "text" in p), "").strip()
        return parse_scores(text)

    for key_idx, api_key in enumerate(GEMINI_KEYS):
        label = f"Gemini key{key_idx+1}"
        for attempt in range(3):
            try:
                apply_scores(call_gemini(api_key))
                print(f"  âœ… è¯„åˆ†å®Œæˆï¼ˆ{label}ï¼‰")
                return articles
            except Exception as e:
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    if attempt < 2:
                        time.sleep((attempt + 1) * 10)
                        print(f"  â³ {label} é™é€Ÿï¼Œé‡è¯•ä¸­...")
                    else:
                        print(f"  â³ {label} æŒç»­é™é€Ÿï¼Œæ¢ä¸‹ä¸€ä¸ª key")
                else:
                    print(f"  âš ï¸  {label}: {e}ï¼Œæ¢ä¸‹ä¸€ä¸ª key")
                    break

    # 2. Groq
    if GROQ_API_KEY:
        try:
            payload = json.dumps({
                "model": "llama-3.3-70b-versatile",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1000,
            }).encode()
            req = Request("https://api.groq.com/openai/v1/chat/completions", data=payload,
                headers={"Authorization": f"Bearer {GROQ_API_KEY}",
                         "Content-Type": "application/json", "User-Agent": "curl/7.88.1"})
            with urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            apply_scores(parse_scores(result["choices"][0]["message"]["content"].strip()))
            print("  âœ… è¯„åˆ†å®Œæˆï¼ˆGroqï¼‰")
            return articles
        except Exception as e:
            print(f"  âš ï¸  Groq: {e}")

    # 3. OpenRouter
    if OPENROUTER_API_KEY:
        for attempt in range(3):
            try:
                payload = json.dumps({
                    "model": "meta-llama/llama-3.3-70b-instruct:free",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 1000,
                }).encode()
                req = Request("https://openrouter.ai/api/v1/chat/completions", data=payload,
                    headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}",
                             "Content-Type": "application/json", "HTTP-Referer": "https://openclaw.ai"})
                with urlopen(req, timeout=30) as resp:
                    result = json.loads(resp.read())
                apply_scores(parse_scores(result["choices"][0]["message"]["content"].strip()))
                print("  âœ… è¯„åˆ†å®Œæˆï¼ˆOpenRouterï¼‰")
                return articles
            except Exception as e:
                if "429" in str(e):
                    time.sleep((attempt + 1) * 15)
                else:
                    print(f"  âš ï¸  OpenRouter: {e}"); break

    print("  âš ï¸  æ‰€æœ‰è¯„åˆ†æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
    for a in articles:
        a["score"] = "â˜…â˜…â˜…â˜†â˜† æš‚æ— è¯„åˆ†"
    return articles

# â”€â”€ å†™å…¥ Google Sheets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_to_sheets(articles):
    if not articles:
        print("æ²¡æœ‰æ–°æ–‡ç« ã€‚"); return

    # æŒ‰æ—¥æœŸåˆ†ç»„ï¼ŒåŒä¸€å¤©å†…æŒ‰é¢†åŸŸæ’åºï¼Œä¸åŒæ—¥æœŸä¹‹é—´ç©ºä¸€è¡Œ
    from collections import defaultdict
    dates_order, by_date = [], defaultdict(list)
    for a in articles:
        if a["date"] not in by_date:
            dates_order.append(a["date"])
        by_date[a["date"]].append(a)

    rows = []
    for i, date in enumerate(sorted(dates_order)):
        day_articles = sorted(by_date[date], key=lambda x: x["field"])
        for a in day_articles:
            rows.append(["'" + a["date"], a["field"], a["journal"],
                         a["authors"], a["title"], a["score"], a["link"]])
        if i < len(dates_order) - 1:
            rows.append(["", "", "", "", "", "", ""])

    # GitHub Actions ç”¨ gspreadï¼ˆService Accountï¼‰ï¼Œæœ¬åœ°ç”¨ gog
    sa_json = os.environ.get("GOOGLE_SERVICE_ACCOUNT", "")
    if sa_json:
        try:
            import gspread
            from google.oauth2.service_account import Credentials
            sa_info = json.loads(sa_json)
            creds = Credentials.from_service_account_info(
                sa_info,
                scopes=["https://www.googleapis.com/auth/spreadsheets"]
            )
            gc = gspread.authorize(creds)
            ws = gc.open_by_key(SHEET_ID).worksheet(SHEET_RANGE)
            ws.append_rows(rows, value_input_option="USER_ENTERED",
                           insert_data_option="INSERT_ROWS")
            print(f"âœ… æˆåŠŸå†™å…¥ {len(articles)} ç¯‡æ–‡ç« åˆ° Google Sheetsï¼ˆgspreadï¼‰")
        except Exception as e:
            print(f"âŒ gspread å†™å…¥å¤±è´¥: {e}")
    else:
        values_json = json.dumps(rows, ensure_ascii=False)
        cmd = ["gog", "sheets", "append", SHEET_ID, SHEET_RANGE,
               "--values-json", values_json, "--insert", "INSERT_ROWS"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… æˆåŠŸå†™å…¥ {len(articles)} ç¯‡æ–‡ç« åˆ° Google Sheetsï¼ˆgogï¼‰")
            print(result.stdout.strip())
        else:
            print(f"âŒ å†™å…¥å¤±è´¥: {result.stderr.strip()}")

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print(f"ğŸ” æŠ“å–æ—¥æœŸ: {TARGET_DATE}")
    print(f"ğŸ“š {len(JOURNALS)} ä¸ªå›½é™…æœŸåˆŠï¼ˆCrossRefï¼‰+ {len(CHINESE_JOURNALS)} ä¸ªä¸­æ–‡æœŸåˆŠï¼ˆRSSï¼‰\n")

    all_articles = []

    with ThreadPoolExecutor(max_workers=10) as ex:
        futures = {ex.submit(fetch_crossref, n, f, i): n for n, f, i in JOURNALS}
        for future in as_completed(futures):
            all_articles.extend(future.result())

    with ThreadPoolExecutor(max_workers=5) as ex:
        futures = {ex.submit(fetch_rss, n, f, u): n for n, f, u in CHINESE_JOURNALS}
        for future in as_completed(futures):
            all_articles.extend(future.result())

    print(f"\nğŸ“ å…±æ‰¾åˆ° {len(all_articles)} ç¯‡æ˜¨å¤©çš„æ–‡ç« ")
    if not all_articles:
        print("æ²¡æœ‰æ–°æ–‡ç« ï¼Œé€€å‡ºã€‚"); return

    print("ğŸ¤– æ­£åœ¨è¯„åˆ†ï¼ˆå•æ¬¡ LLM è°ƒç”¨ï¼‰...")
    all_articles = score_articles(all_articles)

    print("ğŸ“Š å†™å…¥ Google Sheets...")
    write_to_sheets(all_articles)

if __name__ == "__main__":
    main()
