#!/usr/bin/env python3
"""
Think Tank Report Fetcher â€” RSS Edition
æ¯å¤©æŠ“å–ä¸»è¦æ™ºåº“æœ€æ–°æŠ¥å‘Š â†’ å†™å…¥ Google Sheetsã€Œæ™ºåº“æŠ¥å‘Šã€æ ‡ç­¾
"""
import json, os, re, time, base64, functools
from datetime import datetime, timedelta, timezone
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
import xml.etree.ElementTree as ET

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SGT         = timezone(timedelta(hours=8))  # æ–°åŠ å¡æ—¶é—´ (SGT)
_now        = datetime.now(SGT)
# æ­£å¸¸è¿è¡ŒæŠ“æ˜¨å¤©ï¼›æµ‹è¯•/è¡¥æŠ“æ—¶å¯è®¾ LOOKBACK_DAYS=7 ç­‰
LOOKBACK_DAYS = int(os.environ.get("LOOKBACK_DAYS", "1"))
DATE_FROM   = (_now - timedelta(days=LOOKBACK_DAYS)).strftime("%Y-%m-%d")
# LOOKBACK_DAYS=1 æ—¶åªæŠ“æ˜¨å¤©ï¼›>1 æ—¶åŒ…å«ä»Šå¤©ï¼ˆæ–¹ä¾¿æµ‹è¯•éªŒè¯ï¼‰
DATE_TO     = _now.strftime("%Y-%m-%d") if LOOKBACK_DAYS > 1 else (_now - timedelta(days=1)).strftime("%Y-%m-%d")

SHEET_ID  = "1MCcEqV2OGkxFofWSRI6BW2OFYG35cNDHC2olbm43NWc"
SHEET_TAB = "æŠ¥å‘Š"

GEMINI_KEYS = [k for k in [
    os.environ.get("GEMINI_API_KEY", ""),
    os.environ.get("GEMINI_API_KEY_2", ""),
    os.environ.get("GEMINI_API_KEY_3", ""),
] if k]
GROQ_API_KEY       = os.environ.get("GROQ_API_KEY", "")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")

# â”€â”€ Gemini åŠ¨æ€æ¨¡å‹é€‰æ‹© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GEMINI_PREFERRED = [
    "gemini-2.5-flash",       # é¦–é€‰ï¼šæœ€æ–°æœ€ä¼˜ flash
    "gemini-2.0-flash",       # å¤‡é€‰ï¼šä¸Šä¸€ä»£ï¼Œæç¨³å®š
    "gemini-2.0-flash-lite",  # å†å¤‡ï¼šæ›´ä¾¿å®œ
    "gemini-1.5-flash",       # å…œåº•ï¼šè€ä½†æå¯é 
    "gemini-1.5-flash-8b",    # æœ€ç»ˆå…œåº•ï¼šæœ€ä¾¿å®œ
]
_EXCLUDE_KEYWORDS = ("pro", "preview", "exp", "thinking")

def _model_version_key(name):
    m = re.search(r'gemini-(\d+)[.\-](\d+)', name)
    return (int(m.group(1)), int(m.group(2))) if m else (0, 0)

@functools.lru_cache(maxsize=8)
def _list_gemini_models(api_key):
    """åˆ—å‡ºæŒ‡å®š API key å¯ç”¨çš„ Gemini æ¨¡å‹ï¼ˆçº¯ RESTï¼Œä¸ä¾èµ– SDKï¼Œç»“æœç¼“å­˜ï¼‰"""
    try:
        from urllib.request import urlopen as _urlopen
        import json as _json
        url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}&pageSize=200"
        with _urlopen(url, timeout=10) as r:
            data = _json.loads(r.read())
        return frozenset(
            m["name"].removeprefix("models/")
            for m in data.get("models", [])
            if "generateContent" in m.get("supportedGenerationMethods", [])
        )
    except Exception as e:
        print(f"  âš ï¸ æ— æ³•åˆ—å‡º Gemini æ¨¡å‹: {e}")
        return frozenset()

def get_best_gemini_model(api_key):
    """æŒ‰ä¼˜å…ˆçº§é€‰æ‹©æœ€ä½³å¯ç”¨ flash æ¨¡å‹ï¼Œæ’é™¤ pro/preview/exp/thinking"""
    available = _list_gemini_models(api_key)
    if not available:
        return "gemini-2.0-flash"  # åˆ—è¡¨å¤±è´¥æ—¶çš„é»˜è®¤å€¼
    for model in GEMINI_PREFERRED:
        if model in available:
            return model
    # æ‰€æœ‰ä¼˜å…ˆæ¨¡å‹å‡ä¸å¯ç”¨ï¼šè‡ªåŠ¨å¯»æ‰¾ç‰ˆæœ¬æœ€é«˜çš„ flash æ¨¡å‹
    candidates = [
        m for m in available
        if "flash" in m and not any(kw in m for kw in _EXCLUDE_KEYWORDS)
    ]
    if candidates:
        chosen = max(candidates, key=_model_version_key)
        print(f"  ğŸ“Œ è‡ªåŠ¨é™çº§è‡³: {chosen}")
        return chosen
    return "gemini-1.5-flash"

# â”€â”€ Think Tank RSS Feeds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (æœºæ„å, åˆ†ç±»æ ‡ç­¾, RSS URL)
# å·²ç§»é™¤ KFF å’Œ Urban Institute
THINK_TANKS = [
    ("Pew Research Center",          "ç¤¾ä¼šè°ƒç ”", "https://www.pewresearch.org/feed/"),
    ("CEPR",                         "ç»æµæ”¿ç­–", "https://cepr.org/rss.xml"),
    ("Our World in Data",            "å…¨çƒæ•°æ®", "https://ourworldindata.org/atom.xml"),
    ("Our World in Data (Insights)", "å…¨çƒæ•°æ®", "https://ourworldindata.org/atom-data-insights.xml"),
    ("Council on Foreign Relations", "å›½é™…å…³ç³»", "https://feeds.cfr.org/cfr/publications"),
    ("UN News",                      "å›½é™…äº‹åŠ¡", "https://www.un.org/en/rss.xml"),
    ("Aspen Institute",              "æ”¿ç­–ç§‘æŠ€", "https://www.aspeninstitute.org/feed/"),
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                  "KHTML, like Gecko Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/rss+xml, application/xml, text/xml, */*",
}

NS_ATOM = "{http://www.w3.org/2005/Atom}"
NS_DC   = "{http://purl.org/dc/elements/1.1/}"

# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def norm_date(date_str):
    """è§£æå„ç§æ—¥æœŸæ ¼å¼ â†’ YYYY-MM-DDï¼ˆæ–°åŠ å¡æ—¶é—´ï¼‰"""
    if not date_str:
        return ""
    import email.utils
    date_str = date_str.strip()
    try:
        parsed = email.utils.parsedate_to_datetime(date_str)
        return parsed.astimezone(SGT).strftime("%Y-%m-%d")
    except:
        pass
    try:
        cleaned = date_str.replace("Z", "+00:00")
        parsed = datetime.fromisoformat(cleaned)
        return parsed.astimezone(SGT).strftime("%Y-%m-%d")
    except:
        pass
    if len(date_str) >= 10 and date_str[4] == "-" and date_str[7] == "-":
        return date_str[:10]
    return ""

def get_text(el):
    """å®‰å…¨æå– XML å…ƒç´ æ–‡æœ¬"""
    if el is None:
        return ""
    text = (el.text or "").strip()
    if not text:
        text = "".join(el.itertext()).strip()
    return re.sub(r'<[^>]+>', '', text).strip()

_SKIP_TITLES = [
    "acknowledgments", "acknowledgements", "methodology", "appendix",
    "errata", "correction", "about this report", "about this survey",
    "about pew research", "topline questionnaire", "survey questions",
    "codebook", "about the data", "note on",
]

def is_supplementary(title):
    """è¿‡æ»¤é™„å½•ç­‰æ­£å¼æŠ¥å‘Šä¹‹å¤–çš„é¡µé¢"""
    t = title.lower().strip()
    if t in _SKIP_TITLES:
        return True
    if any(t.startswith(kw) for kw in ("appendix", "errata:", "correction:")):
        return True
    return False

def get_atom_link(item):
    """ä» Atom entry æå–é“¾æ¥"""
    for link_el in item.findall(f"{NS_ATOM}link"):
        rel  = link_el.get("rel", "alternate")
        href = link_el.get("href", "")
        if rel in ("alternate", "") and href:
            return href
    link_el = item.find(f"{NS_ATOM}link")
    return (link_el.text or "").strip() if link_el is not None else ""

# â”€â”€ RSS æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_think_tank(name, category, url):
    try:
        req = Request(url, headers=HEADERS)
        with urlopen(req, timeout=15) as resp:
            raw = resp.read()
        content = raw.decode("utf-8", errors="replace").lstrip("\ufeff")
        root = ET.fromstring(content.encode("utf-8"))

        is_atom = (root.tag == f"{NS_ATOM}feed" or
                   root.find(f".//{NS_ATOM}entry") is not None)

        if is_atom:
            items = root.findall(f".//{NS_ATOM}entry")
        else:
            items = root.findall(".//item")

        articles = []
        for item in items:
            if is_atom:
                title_el = item.find(f"{NS_ATOM}title")
                _upd     = item.find(f"{NS_ATOM}updated")
                date_el  = _upd if _upd is not None else item.find(f"{NS_ATOM}published")
                link     = get_atom_link(item)
            else:
                title_el = item.find("title")
                _pub     = item.find("pubDate")
                date_el  = _pub if _pub is not None else item.find(f"{NS_DC}date")
                link_el  = item.find("link")
                link     = get_text(link_el) if link_el is not None else ""

            title    = get_text(title_el)
            pub_date = norm_date(get_text(date_el))

            if not title or not pub_date or pub_date < DATE_FROM or pub_date > DATE_TO:
                continue
            if is_supplementary(title):
                continue

            articles.append({
                "source":   name,
                "category": category,
                "title":    title,
                "date":     pub_date,
                "link":     link,
            })

        print(f"  âœ… {name}: {len(articles)} ç¯‡")
        return articles

    except HTTPError as e:
        print(f"  âš ï¸  {name}: HTTP {e.code}")
        return []
    except Exception as e:
        print(f"  âš ï¸  {name}: å¤±è´¥ ({e})")
        return []

# â”€â”€ LLM ä¸­æ–‡ç®€ä»‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_reports(articles):
    if not articles:
        return articles

    titles_list = "\n".join([
        f"{i+1}. [{a['source']}] {a['title']}" for i, a in enumerate(articles)
    ])
    prompt = f"""ä½ æ˜¯ä¸€ä½ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„ç¼–è¾‘ï¼Œè´Ÿè´£ä¸ºç¤¾ä¼šå­¦å…¬ä¼—å·ç­›é€‰æ™ºåº“æŠ¥å‘Šã€‚
è¯·å¯¹ä»¥ä¸‹æ ‡é¢˜å®Œæˆï¼š
1. åˆ¤æ–­ç›¸å…³æ€§ï¼ˆrelevant true/falseï¼‰
2. è‹¥ç›¸å…³ï¼Œç”¨ä¸€å¥ä¸­æ–‡ç®€ä»‹ï¼ˆ35å­—ä»¥å†…ï¼‰ï¼›ä¸ç›¸å…³ score ç•™ç©ºã€‚
å…·ä½“è§„åˆ™å‚è€ƒç¤¾ä¼šå­¦ã€å®è§‚æ”¿ç­–ã€‚

åˆ—è¡¨ï¼š
{titles_list}

è¯·ä¸¥æ ¼æŒ‰ JSON è¿”å›ï¼š
[
  {{"index": 1, "relevant": true,  "score": "ç®€ä»‹æ–‡æœ¬"}},
  ...
]"""

    def parse_scores(content):
        content = content.strip()
        if content.startswith("```"):
            content = content.split("\n", 1)[-1].rsplit("```", 1)[0]
        start, end = content.find("["), content.rfind("]") + 1
        return json.loads(content[start:end])

    def apply_scores(scores):
        score_map    = {s["index"]: s.get("score", "æš‚æ— ç®€ä»‹") for s in scores}
        relevant_set = {s["index"] for s in scores if s.get("relevant", True)}
        for i, a in enumerate(articles):
            a["intro"]    = score_map.get(i + 1, "æš‚æ— ç®€ä»‹")
            a["relevant"] = (i + 1) in relevant_set

    # 1. Groqï¼ˆé»˜è®¤ï¼Œæœ€ç¨³å®šï¼‰
    if GROQ_API_KEY:
        try:
            payload = json.dumps({
                "model": "llama-3.3-70b-versatile",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1000,
            }).encode()
            req = Request("https://api.groq.com/openai/v1/chat/completions", data=payload,
                headers={"Authorization": f"Bearer {GROQ_API_KEY}",
                         "Content-Type": "application/json", "User-Agent": "curl/7.88.1"})
            with urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            apply_scores(parse_scores(result["choices"][0]["message"]["content"].strip()))
            print("  âœ… ç®€ä»‹ç”Ÿæˆå®Œæˆï¼ˆGroqï¼‰")
            return _filter_relevant(articles)
        except Exception as e:
            print(f"  âš ï¸  Groq: {e}ï¼Œå°è¯• Gemini...")

    # 2. Geminiï¼ˆå¤‡ç”¨ï¼‰
    def call_gemini(api_key):
        model = get_best_gemini_model(api_key)
        print(f"  ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
        payload = json.dumps({
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"maxOutputTokens": 2000}
        }).encode()
        req = Request(
            f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}",
            data=payload, headers={"Content-Type": "application/json"},
        )
        with urlopen(req, timeout=60) as resp:
            result = json.loads(resp.read())
        parts = result["candidates"][0]["content"]["parts"]
        text = next((p["text"] for p in reversed(parts) if "text" in p), "").strip()
        return parse_scores(text)

    for key_idx, api_key in enumerate(GEMINI_KEYS):
        label = f"Gemini key{key_idx+1}"
        for attempt in range(3):
            try:
                apply_scores(call_gemini(api_key))
                print(f"  âœ… ç®€ä»‹ç”Ÿæˆå®Œæˆï¼ˆ{label}ï¼‰")
                return _filter_relevant(articles)
            except Exception as e:
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    if attempt < 2:
                        time.sleep((attempt + 1) * 10)
                        print(f"  â³ {label} é™é€Ÿï¼Œé‡è¯•ä¸­...")
                    else:
                        print(f"  â³ {label} æŒç»­é™é€Ÿï¼Œæ¢ä¸‹ä¸€ä¸ª key")
                else:
                    print(f"  âš ï¸  {label}: {e}ï¼Œæ¢ä¸‹ä¸€ä¸ª key")
                    break

    # 3. OpenRouter
    if OPENROUTER_API_KEY:
        for attempt in range(3):
            try:
                payload = json.dumps({
                    "model": "meta-llama/llama-3.3-70b-instruct:free",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 1000,
                }).encode()
                req = Request("https://openrouter.ai/api/v1/chat/completions", data=payload,
                    headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}",
                             "Content-Type": "application/json", "HTTP-Referer": "https://openclaw.ai"})
                with urlopen(req, timeout=30) as resp:
                    result = json.loads(resp.read())
                apply_scores(parse_scores(result["choices"][0]["message"]["content"].strip()))
                print("  âœ… ç®€ä»‹ç”Ÿæˆå®Œæˆï¼ˆOpenRouterï¼‰")
                return _filter_relevant(articles)
            except Exception as e:
                if "429" in str(e):
                    time.sleep((attempt + 1) * 15)
                else:
                    print(f"  âš ï¸  OpenRouter: {e}"); break

    print("  âš ï¸  æ‰€æœ‰æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    for a in articles:
        a["intro"] = a.get("intro", "æš‚æ— ç®€ä»‹")
        a["relevant"] = a.get("relevant", True)
    return articles

def _filter_relevant(articles):
    kept = [a for a in articles if a.get("relevant", True)]
    print(f"  ğŸ” ä¿ç•™ {len(kept)}/{len(articles)} ç¯‡æŠ¥å‘Š")
    return kept

# â”€â”€ å†™å…¥ Google Sheets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_to_sheets(articles):
    if not articles: return
    rows = []
    for a in sorted(articles, key=lambda x: x["category"]):
        rows.append(["'" + a["date"], a["category"], a["source"],
                     a["title"], a["intro"], a["link"]])

    sa_json = os.environ.get("GOOGLE_SERVICE_ACCOUNT", "")
    try:
        import gspread
        from google.oauth2.service_account import Credentials

        if sa_json:
            # æœ¬åœ°/GitHub Actionsï¼šä½¿ç”¨ JSON keyï¼ˆBase64 æˆ–åŸå§‹ JSONï¼‰
            sa_info = json.loads(base64.b64decode(sa_json))
            creds = Credentials.from_service_account_info(
                sa_info, scopes=["https://www.googleapis.com/auth/spreadsheets"]
            )
        else:
            # GCP Cloud Runï¼šä½¿ç”¨ Application Default Credentials
            import google.auth
            creds, _ = google.auth.default(
                scopes=["https://www.googleapis.com/auth/spreadsheets"])

        gc = gspread.authorize(creds)
        ws = gc.open_by_key(SHEET_ID).worksheet(SHEET_TAB)
        # æ—¶é—´æˆ³è¡Œ + æ•°æ® + ç©ºè¡Œåˆ†éš”ï¼ˆç½®é¡¶ï¼‰
        ts = datetime.now(SGT).strftime("%Y/%m/%d, %H:%M") + "å®Œæˆæ›´æ–°"
        timestamp_row = [[ts] + [""] * (len(rows[0]) - 1)]
        separator = [[""] * len(rows[0])]
        ws.insert_rows(timestamp_row + rows + separator, row=2, value_input_option="USER_ENTERED")
        print(f"âœ… æˆåŠŸå†™å…¥ {len(articles)} ç¯‡æŠ¥å‘Šï¼ˆå·²ç½®é¡¶ï¼‰")
    except Exception as e:
        print(f"âŒ gspread å†™å…¥å¤±è´¥: {e}")

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print(f"ğŸ” æŠ“å–èŒƒå›´: {DATE_FROM} è‡³ {DATE_TO}")
    all_articles = []
    for name, category, url in THINK_TANKS:
        all_articles.extend(fetch_think_tank(name, category, url))
        time.sleep(0.5)

    if not all_articles:
        print("æ²¡æœ‰æ–°æŠ¥å‘Šï¼Œé€€å‡ºã€‚"); return

    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆç®€ä»‹...")
    all_articles = summarize_reports(all_articles)
    
    print("ğŸ“Š å†™å…¥ Google Sheets...")
    write_to_sheets(all_articles)

if __name__ == "__main__":
    main()
