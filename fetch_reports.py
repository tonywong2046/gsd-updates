#!/usr/bin/env python3
"""
Think Tank Report Fetcher â€” RSS Edition
æ¯å¤©æŠ“å–ä¸»è¦æ™ºåº“æœ€æ–°æŠ¥å‘Š â†’ å†™å…¥ Google Sheetsã€Œæ™ºåº“æŠ¥å‘Šã€æ ‡ç­¾
"""
import json, os, re, time, base64
from datetime import datetime, timedelta, timezone
from urllib.request import urlopen, Request
from urllib.error import HTTPError, URLError
import xml.etree.ElementTree as ET

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SGT         = timezone(timedelta(hours=8))  # æ–°åŠ å¡æ—¶é—´ (SGT)
_now        = datetime.now(SGT)
# æ­£å¸¸è¿è¡ŒæŠ“æ˜¨å¤©ï¼›æµ‹è¯•/è¡¥æŠ“æ—¶å¯è®¾ LOOKBACK_DAYS=7 ç­‰
LOOKBACK_DAYS = int(os.environ.get("LOOKBACK_DAYS", "1"))
DATE_FROM   = (_now - timedelta(days=LOOKBACK_DAYS)).strftime("%Y-%m-%d")
# LOOKBACK_DAYS=1 æ—¶åªæŠ“æ˜¨å¤©ï¼›>1 æ—¶åŒ…å«ä»Šå¤©ï¼ˆæ–¹ä¾¿æµ‹è¯•éªŒè¯ï¼‰
DATE_TO     = _now.strftime("%Y-%m-%d") if LOOKBACK_DAYS > 1 else (_now - timedelta(days=1)).strftime("%Y-%m-%d")

SHEET_ID  = "1MCcEqV2OGkxFofWSRI6BW2OFYG35cNDHC2olbm43NWc"
SHEET_TAB = "æŠ¥å‘Š"

GEMINI_KEYS = [k for k in [
    os.environ.get("GEMINI_API_KEY", ""),
    os.environ.get("GEMINI_API_KEY_2", ""),
    os.environ.get("GEMINI_API_KEY_3", ""),
] if k]
GROQ_API_KEY       = os.environ.get("GROQ_API_KEY", "")
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY", "")

# â”€â”€ Think Tank RSS Feeds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (æœºæ„å, åˆ†ç±»æ ‡ç­¾, RSS URL)
THINK_TANKS = [
    ("Pew Research Center",          "ç¤¾ä¼šè°ƒç ”", "https://www.pewresearch.org/feed/"),
    ("KFF",                          "åŒ»ç–—æ”¿ç­–", "https://kff.org/feed/"),
    ("Urban Institute",              "ç¤¾ä¼šæ”¿ç­–", "https://www.urban.org/research/rss.xml"),
    ("CEPR",                         "ç»æµæ”¿ç­–", "https://cepr.org/rss.xml"),
    ("Our World in Data",            "å…¨çƒæ•°æ®", "https://ourworldindata.org/atom.xml"),
    ("Council on Foreign Relations", "å›½é™…å…³ç³»", "https://feeds.cfr.org/cfr/publications"),
    ("UN News",                      "å›½é™…äº‹åŠ¡", "https://www.un.org/en/rss.xml"),
    ("Aspen Institute",              "æ”¿ç­–ç§‘æŠ€", "https://www.aspeninstitute.org/feed/"),
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
                  "KHTML, like Gecko Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/rss+xml, application/xml, text/xml, */*",
}

NS_ATOM = "{http://www.w3.org/2005/Atom}"
NS_DC   = "{http://purl.org/dc/elements/1.1/}"

# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def norm_date(date_str):
    """è§£æå„ç§æ—¥æœŸæ ¼å¼ â†’ YYYY-MM-DDï¼ˆæ–°åŠ å¡æ—¶é—´ï¼‰"""
    if not date_str:
        return ""
    import email.utils
    date_str = date_str.strip()
    try:
        parsed = email.utils.parsedate_to_datetime(date_str)
        return parsed.astimezone(SGT).strftime("%Y-%m-%d")
    except:
        pass
    try:
        cleaned = date_str.replace("Z", "+00:00")
        parsed = datetime.fromisoformat(cleaned)
        return parsed.astimezone(SGT).strftime("%Y-%m-%d")
    except:
        pass
    if len(date_str) >= 10 and date_str[4] == "-" and date_str[7] == "-":
        return date_str[:10]
    return ""

def get_text(el):
    """å®‰å…¨æå– XML å…ƒç´ æ–‡æœ¬ï¼ˆå¤„ç† CDATAã€åµŒå¥—æ ‡ç­¾ï¼‰"""
    if el is None:
        return ""
    text = (el.text or "").strip()
    if not text:
        text = "".join(el.itertext()).strip()
    return re.sub(r'<[^>]+>', '', text).strip()

_SKIP_TITLES = [
    "acknowledgments", "acknowledgements", "methodology", "appendix",
    "errata", "correction", "about this report", "about this survey",
    "about pew research", "topline questionnaire", "survey questions",
    "codebook", "about the data", "note on",
]

def is_supplementary(title):
    """è¿‡æ»¤é™„å½•ã€æ–¹æ³•è®ºã€è‡´è°¢ç­‰é™„å±é¡µé¢ï¼Œåªä¿ç•™æ­£å¼æŠ¥å‘Š"""
    t = title.lower().strip()
    # å®Œæ•´åŒ¹é…ï¼ˆæ ‡é¢˜å°±æ˜¯è¿™ä¸ªè¯ï¼‰
    if t in _SKIP_TITLES:
        return True
    # å‰ç¼€åŒ¹é…ï¼ˆå¦‚ "Appendix A: ..."ã€"Appendix E: Detailed tables"ï¼‰
    if any(t.startswith(kw) for kw in ("appendix", "errata:", "correction:")):
        return True
    return False

def get_atom_link(item):
    """ä» Atom entry æå– alternate é“¾æ¥"""
    for link_el in item.findall(f"{NS_ATOM}link"):
        rel  = link_el.get("rel", "alternate")
        href = link_el.get("href", "")
        if rel in ("alternate", "") and href:
            return href
    link_el = item.find(f"{NS_ATOM}link")
    return (link_el.text or "").strip() if link_el is not None else ""

# â”€â”€ RSS æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_think_tank(name, category, url):
    try:
        req = Request(url, headers=HEADERS)
        with urlopen(req, timeout=15) as resp:
            raw = resp.read()
        content = raw.decode("utf-8", errors="replace").lstrip("\ufeff")
        root = ET.fromstring(content.encode("utf-8"))

        # åˆ¤æ–­æ˜¯ Atom è¿˜æ˜¯ RSS 2.0
        is_atom = (root.tag == f"{NS_ATOM}feed" or
                   root.find(f".//{NS_ATOM}entry") is not None)

        if is_atom:
            items = root.findall(f".//{NS_ATOM}entry")
        else:
            items = root.findall(".//item")

        articles = []
        for item in items:
            if is_atom:
                title_el = item.find(f"{NS_ATOM}title")
                _upd     = item.find(f"{NS_ATOM}updated")
                date_el  = _upd if _upd is not None else item.find(f"{NS_ATOM}published")
                link     = get_atom_link(item)
            else:
                title_el = item.find("title")
                _pub     = item.find("pubDate")
                date_el  = _pub if _pub is not None else item.find(f"{NS_DC}date")
                link_el  = item.find("link")
                link     = get_text(link_el) if link_el is not None else ""

            title    = get_text(title_el)
            pub_date = norm_date(get_text(date_el))

            if not title or not pub_date or pub_date < DATE_FROM or pub_date > DATE_TO:
                continue
            if is_supplementary(title):
                continue

            articles.append({
                "source":   name,
                "category": category,
                "title":    title,
                "date":     pub_date,
                "link":     link,
            })

        print(f"  âœ… {name}: {len(articles)} ç¯‡")
        return articles

    except HTTPError as e:
        print(f"  âš ï¸  {name}: HTTP {e.code}")
        return []
    except Exception as e:
        print(f"  âš ï¸  {name}: å¤±è´¥ ({e})")
        return []

# â”€â”€ LLM ä¸­æ–‡ç®€ä»‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_reports(articles):
    if not articles:
        return articles

    titles_list = "\n".join([
        f"{i+1}. [{a['source']}] {a['title']}" for i, a in enumerate(articles)
    ])
    prompt = f"""ä½ æ˜¯ä¸€ä½æ”¿ç­–ç ”ç©¶ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯æ¥è‡ªå„å¤§æ™ºåº“çš„æœ€æ–°æŠ¥å‘Šæ ‡é¢˜ï¼Œè¯·æ ¹æ®æ ‡é¢˜é€ä¸€ç”¨ä¸€å¥ä¸­æ–‡è¯´æ˜è¿™ä»½æŠ¥å‘Šå¤§æ¦‚åœ¨ç ”ç©¶ä»€ä¹ˆã€‚

è¦æ±‚ï¼š
- åªæ ¹æ®æ ‡é¢˜æ¨æ–­ï¼Œä¸è¦ç¼–é€ å†…å®¹
- æ¯æ¡ç®€ä»‹æ§åˆ¶åœ¨35å­—ä»¥å†…
- è¯­è¨€ç®€æ´ï¼Œç›´æ¥è¯´æ˜ç ”ç©¶ä¸»é¢˜

æŠ¥å‘Šåˆ—è¡¨ï¼š
{titles_list}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ï¼š
[
  {{"index": 1, "score": "ä¸€å¥è¯ä¸­æ–‡ç®€ä»‹"}},
  {{"index": 2, "score": "ä¸€å¥è¯ä¸­æ–‡ç®€ä»‹"}}
]"""

    def parse_scores(content):
        content = content.strip()
        if content.startswith("```"):
            content = content.split("\n", 1)[-1].rsplit("```", 1)[0]
        start, end = content.find("["), content.rfind("]") + 1
        if start == -1 or end == 0:
            raise ValueError(f"No JSON array: {content[:80]!r}")
        return json.loads(content[start:end])

    def apply_scores(scores):
        score_map = {s["index"]: s["score"] for s in scores}
        for i, a in enumerate(articles):
            a["intro"] = score_map.get(i + 1, "æš‚æ— ç®€ä»‹")

    # 1. Gemini
    def call_gemini(api_key):
        payload = json.dumps({
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {"maxOutputTokens": 2000, "thinkingConfig": {"thinkingBudget": 0}},
        }).encode()
        req = Request(
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}",
            data=payload, headers={"Content-Type": "application/json"},
        )
        with urlopen(req, timeout=60) as resp:
            result = json.loads(resp.read())
        parts = result["candidates"][0]["content"]["parts"]
        text = next((p["text"] for p in reversed(parts) if "text" in p), "").strip()
        return parse_scores(text)

    for key_idx, api_key in enumerate(GEMINI_KEYS):
        label = f"Gemini key{key_idx+1}"
        for attempt in range(3):
            try:
                apply_scores(call_gemini(api_key))
                print(f"  âœ… ç®€ä»‹ç”Ÿæˆå®Œæˆï¼ˆ{label}ï¼‰")
                return articles
            except Exception as e:
                if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                    if attempt < 2:
                        time.sleep((attempt + 1) * 10)
                        print(f"  â³ {label} é™é€Ÿï¼Œé‡è¯•ä¸­...")
                    else:
                        print(f"  â³ {label} æŒç»­é™é€Ÿï¼Œæ¢ä¸‹ä¸€ä¸ª key")
                else:
                    print(f"  âš ï¸  {label}: {e}ï¼Œæ¢ä¸‹ä¸€ä¸ª key")
                    break

    # 2. Groq
    if GROQ_API_KEY:
        try:
            payload = json.dumps({
                "model": "llama-3.3-70b-versatile",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 1500,
            }).encode()
            req = Request("https://api.groq.com/openai/v1/chat/completions", data=payload,
                headers={"Authorization": f"Bearer {GROQ_API_KEY}",
                         "Content-Type": "application/json", "User-Agent": "curl/7.88.1"})
            with urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
            apply_scores(parse_scores(result["choices"][0]["message"]["content"].strip()))
            print("  âœ… ç®€ä»‹ç”Ÿæˆå®Œæˆï¼ˆGroqï¼‰")
            return articles
        except Exception as e:
            print(f"  âš ï¸  Groq: {e}")

    # 3. OpenRouter
    if OPENROUTER_API_KEY:
        for attempt in range(3):
            try:
                payload = json.dumps({
                    "model": "meta-llama/llama-3.3-70b-instruct:free",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": 1500,
                }).encode()
                req = Request("https://openrouter.ai/api/v1/chat/completions", data=payload,
                    headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}",
                             "Content-Type": "application/json"})
                with urlopen(req, timeout=30) as resp:
                    result = json.loads(resp.read())
                apply_scores(parse_scores(result["choices"][0]["message"]["content"].strip()))
                print("  âœ… ç®€ä»‹ç”Ÿæˆå®Œæˆï¼ˆOpenRouterï¼‰")
                return articles
            except Exception as e:
                if "429" in str(e):
                    time.sleep((attempt + 1) * 15)
                else:
                    print(f"  âš ï¸  OpenRouter: {e}"); break

    print("  âš ï¸  æ‰€æœ‰æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç®€ä»‹")
    for a in articles:
        a["intro"] = "æš‚æ— ç®€ä»‹"
    return articles

# â”€â”€ å†™å…¥ Google Sheets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_to_sheets(articles):
    if not articles:
        print("æ²¡æœ‰æ–°æŠ¥å‘Šã€‚"); return

    # æŒ‰åˆ†ç±»æ’åº
    rows = []
    for a in sorted(articles, key=lambda x: x["category"]):
        rows.append(["'" + a["date"], a["category"], a["source"],
                     a["title"], a["intro"], a["link"]])

    sa_json = os.environ.get("GOOGLE_SERVICE_ACCOUNT", "")
    if sa_json:
        try:
            import gspread
            from google.oauth2.service_account import Credentials
            sa_info = json.loads(base64.b64decode(sa_json))
            creds = Credentials.from_service_account_info(
                sa_info, scopes=["https://www.googleapis.com/auth/spreadsheets"]
            )
            gc = gspread.authorize(creds)
            ws = gc.open_by_key(SHEET_ID).worksheet(SHEET_TAB)
            ws.append_rows(rows, value_input_option="USER_ENTERED",
                           insert_data_option="INSERT_ROWS")
            print(f"âœ… æˆåŠŸå†™å…¥ {len(articles)} ç¯‡æŠ¥å‘Šåˆ° Google Sheetsï¼ˆgspreadï¼‰")
        except Exception as e:
            print(f"âŒ gspread å†™å…¥å¤±è´¥: {e}")
    else:
        import subprocess
        values_json = json.dumps(rows, ensure_ascii=False)
        cmd = ["gog", "sheets", "append", SHEET_ID, SHEET_TAB,
               "--values-json", values_json, "--insert", "INSERT_ROWS"]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… æˆåŠŸå†™å…¥ {len(articles)} ç¯‡æŠ¥å‘Šï¼ˆgogï¼‰")
        else:
            print(f"âŒ å†™å…¥å¤±è´¥: {result.stderr.strip()}")

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print(f"ğŸ” æŠ“å–èŒƒå›´: {DATE_FROM} è‡³ {DATE_TO}ï¼ˆ{LOOKBACK_DAYS}å¤©ï¼‰")
    print(f"ğŸ“¡ {len(THINK_TANKS)} ä¸ªæ™ºåº“ RSS æº\n")

    all_articles = []
    for name, category, url in THINK_TANKS:
        articles = fetch_think_tank(name, category, url)
        all_articles.extend(articles)
        time.sleep(0.5)

    print(f"\nğŸ“ å…±æ‰¾åˆ° {len(all_articles)} ç¯‡æŠ¥å‘Š")
    if not all_articles:
        print("æ²¡æœ‰æ–°æŠ¥å‘Šï¼Œé€€å‡ºã€‚"); return

    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆä¸­æ–‡ç®€ä»‹...")
    all_articles = summarize_reports(all_articles)

    print("ğŸ“Š å†™å…¥ Google Sheets...")
    write_to_sheets(all_articles)

if __name__ == "__main__":
    main()
