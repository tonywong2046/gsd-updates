#!/usr/bin/env python3
"""
fetch_jobs.py â€” ä»å¤šä¸ªå­¦æœ¯æ‹›è˜ç½‘ç«™æŠ“å–èŒä½ï¼Œå†™å…¥ Google Sheetsï¼ˆå·¥ä½œ tabï¼‰
æ¥æºï¼šjobs.ac.ukï¼ˆæŒ‰å­¦ç§‘ RSSï¼‰ã€Times Higher Education Jobsï¼ˆå…¨çƒ RSS + å…³é”®è¯è¿‡æ»¤ï¼‰ã€ReliefWeb RSS
ç”¨æ³•ï¼š
  python fetch_jobs.py           # å¢é‡æ¨¡å¼ï¼ˆè·³è¿‡å·²è§èŒä½ï¼‰
  python fetch_jobs.py --all     # å…¨é‡æ¨¡å¼ï¼ˆå¿½ç•¥ seen è®°å½•ï¼Œå†™å…¥å…¨éƒ¨å½“å‰èŒä½ï¼‰
  python fetch_jobs.py --the-only  # åªè·‘ THE Jobsï¼Œå¿«é€Ÿæµ‹è¯•
  python fetch_jobs.py --week    # é™é€Ÿæ¨¡å¼ï¼šjobs.ac.uk æ¯ç§‘åªå–5æ¡ï¼Œç”¨äºæœ¬åœ°éªŒè¯
åˆ—ï¼šå‘ç°æ—¥æœŸ | å­¦ç§‘ | æœºæ„ | èŒä½ | è–ªèµ„ | ç”³è¯·æˆªæ­¢æ—¥æœŸ | ç”³è¯·é“¾æ¥ | æ¥æº
"""

import re, sys, json, html, subprocess, os, time, random
from datetime import datetime, timedelta, timezone
from xml.etree import ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib.request
import gspread
from google.oauth2.service_account import Credentials

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SHEET_ID    = "1MCcEqV2OGkxFofWSRI6BW2OFYG35cNDHC2olbm43NWc"
SHEET_RANGE = "å·¥ä½œ"
SGT         = timezone(timedelta(hours=8))
_now        = datetime.now(SGT)
TODAY       = _now.strftime("%Y-%m-%d")
_date_from  = (_now - timedelta(days=7)).strftime("%Y/%m/%d")
_date_to    = _now.strftime("%Y/%m/%d")
DATE_LABEL  = f"{_date_from}-{_date_to}"
SEEN_FILE   = "/tmp/seen_jobs.json"   # Cloud Run åªæœ‰ /tmp å¯å†™

RESET_ALL   = "--all"      in sys.argv
THE_ONLY    = "--the-only" in sys.argv
WEEK_MODE   = "--week"     in sys.argv   # æ¯å­¦ç§‘åªå–5æ¡ï¼ŒåŠ é€Ÿæœ¬åœ°éªŒè¯

BASE = "https://www.jobs.ac.uk"
RSS_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
    "Accept":     "application/rss+xml, application/xml, text/xml, */*",
}
_CURL_BASE = [
    "curl", "-sL", "--max-time", "20", "--compressed",
    "-H", "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "-H", "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "-H", "Accept-Language: en-GB,en;q=0.9",
    "-H", "Accept-Encoding: gzip, deflate, br",
    "-H", "Connection: keep-alive",
]

# â”€â”€ THE Jobs é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
THE_RSS_FEEDS = [
    ("Sociology",       "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=sociology"),
    ("Social Science",  "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=social+science"),
    ("Politics",        "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=political+science"),
    ("Psychology",      "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=psychology"),
    ("Philosophy",      "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=philosophy"),
    ("History",         "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=history"),
    ("Anthropology",    "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=anthropology"),
    ("Media Studies",   "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=media+studies"),
    ("Cultural Studies","https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=cultural+studies"),
    ("Management",      "https://www.timeshighereducation.com/unijobs/jobsrss/?keywords=management"),
]
THE_DAYS = 9   # è¿‡å»å‡ å¤©å†…å‘å¸ƒçš„èŒä½ï¼ˆå‘¨è·‘ç”¨ 9 å¤©ï¼Œä¿ç•™ç¼“å†²é¿å…æ¼æŠ“ï¼‰

THE_KEYWORD_MAP = [
    ("history of art",          "History of Art"),
    ("art history",             "History of Art"),
    ("human resources",         "Human Resources Management"),
    ("social policy",           "Social Policy"),
    ("social work",             "Social Work"),
    ("social geography",        "Human & Social Geography"),
    ("human geography",         "Human & Social Geography"),
    ("cultural studies",        "Cultural Studies"),
    ("cultural geography",      "Cultural Studies"),
    ("media studies",           "Media & Communications"),
    ("media and communication", "Media & Communications"),
    ("communication studies",   "Media & Communications"),
    ("journalism",              "Media & Communications"),
    ("publishing",              "Media & Communications"),
    ("sociology",               "Sociology"),
    ("anthropolog",             "Anthropology"),
    ("political science",       "Politics & Government"),
    ("politics",                "Politics & Government"),
    ("government",              "Politics & Government"),
    ("international relations", "Politics & Government"),
    ("philosophy",              "Philosophy"),
    ("psychology",              "Psychology"),
    ("history",                 "History"),
    ("management",              "Management"),
    ("business studies",        "Business Studies"),
    ("business school",         "Business Studies"),
    ("criminology",             "Sociology"),
    ("gender studies",          "Sociology"),
    ("social science",          "Other Social Sciences"),
    ("development studies",     "Other Social Sciences"),
    ("public policy",           "Other Social Sciences"),
    ("demography",              "Other Social Sciences"),
]

# â”€â”€ jobs.ac.uk å­¦ç§‘é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBJECT_FEEDS = [
    ("Sociology",                           "/jobs/sociology/?format=rss"),
    ("Anthropology",                        "/jobs/anthropology/?format=rss"),
    ("Social Policy",                       "/jobs/social-policy/?format=rss"),
    ("Social Work",                         "/jobs/social-work/?format=rss"),
    ("Politics & Government",               "/jobs/politics-and-government/?format=rss"),
    ("Cultural Studies",                    "/jobs/cultural-studies/?format=rss"),
    ("Human & Social Geography",            "/jobs/human-and-social-geography/?format=rss"),
    ("Other Social Sciences",               "/jobs/other-social-sciences/?format=rss"),
    ("Business Studies",                    "/jobs/business-studies/?format=rss"),
    ("Human Resources Management",          "/jobs/human-resources-management/?format=rss"),
    ("Management",                          "/jobs/management/?format=rss"),
    ("Other Business & Management Studies", "/jobs/other-business-and-management-studies/?format=rss"),
    ("History",                             "/jobs/history/?format=rss"),
    ("History of Art",                      "/jobs/history-of-art/?format=rss"),
    ("Philosophy",                          "/jobs/philosophy/?format=rss"),
    ("Psychology",                          "/jobs/psychology/?format=rss"),
    ("Media & Communications",              "/jobs/media-studies/?format=rss"),
    ("Media & Communications",              "/jobs/journalism/?format=rss"),
    ("Media & Communications",              "/jobs/communication-studies/?format=rss"),
    ("Media & Communications",              "/jobs/publishing/?format=rss"),
]

# International_Orgs æ”¾æœ€åï¼ˆReliefWeb ä½¿ç”¨ï¼‰
TARGET_SUBJECTS = list(dict.fromkeys(s for s, _ in SUBJECT_FEEDS)) + ["International_Orgs"]

# â”€â”€ ReliefWeb RSS é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RW_RSS_FEEDS = [
    ("Social Sciences", "https://reliefweb.int/jobs/rss.xml?career_category=5&source_active=1"),
    ("Query",           "https://reliefweb.int/jobs/rss.xml?query%5Bvalue%5D=social+science"),
]

# â”€â”€ å·²è§èŒä½è®°å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_seen():
    if RESET_ALL:
        return set()
    try:
        with open(SEEN_FILE) as f:
            return set(json.load(f))
    except Exception:
        return set()

def save_seen(seen):
    try:
        with open(SEEN_FILE, "w") as f:
            json.dump(list(seen), f)
    except Exception as e:
        print(f"âš ï¸  seen_jobs å†™å…¥å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _strip_tags(s):
    return re.sub(r'<[^>]+>', '', s).strip()

def _fix_entities(content_bytes):
    """ä¿®å¤ RSS ä¸­ä¸åˆè§„çš„è£¸ & å®ä½“"""
    return re.sub(
        rb'&(?!(?:amp|lt|gt|quot|apos|#\d+|#x[0-9a-fA-F]+);)',
        rb'&amp;', content_bytes)

def parse_rss_description(desc_raw):
    """RSS description â†’ (æœºæ„å, è–ªèµ„)ï¼ŒåŒé‡ HTML ç¼–ç """
    text = html.unescape(html.unescape(desc_raw))
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    sal_m = re.search(r'Salary\s*[:\-]?\s*(.+)', text, re.IGNORECASE)
    if sal_m:
        salary      = sal_m.group(1).strip()
        institution = text[:sal_m.start()].strip().rstrip('|').strip()
    else:
        salary, institution = "", text.strip()
    return institution, salary

_MONTHS = (r'Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?'
           r'|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?')
_DATE_PAT = rf'\d{{1,2}}\s+(?:{_MONTHS})\s+\d{{4}}'

_MONTHS_MAP = {"january":1,"february":2,"march":3,"april":4,"may":5,"june":6,
               "july":7,"august":8,"september":9,"october":10,"november":11,"december":12}

def _parse_go_live(raw):
    """'20th February 2026' â†’ '2026-02-20'"""
    m = re.match(r'(\d{1,2})(?:st|nd|rd|th)?\s+(\w+)\s+(\d{4})',
                 (raw or "").strip(), re.IGNORECASE)
    if m:
        d, mon, y = int(m.group(1)), m.group(2).lower(), int(m.group(3))
        mo = _MONTHS_MAP.get(mon)
        if mo:
            return f"{y}-{mo:02d}-{d:02d}"
    return ""

def _parse_pubdate(date_str):
    """è§£æ RSS pubDateï¼ˆRFC 2822ï¼‰â†’ å¸¦æ—¶åŒº datetimeï¼›å¤±è´¥è¿”å› None"""
    from email.utils import parsedate_to_datetime
    try:
        return parsedate_to_datetime(date_str.strip())
    except Exception:
        return None

# â”€â”€ HTTPï¼ˆcurlï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _curl_get(url):
    """curl æŠ“é¡µé¢ï¼Œè¿”å› HTML å­—ç¬¦ä¸²ï¼›å¤±è´¥è¿”å›ç©ºä¸²"""
    try:
        result = subprocess.run(_CURL_BASE + [url], capture_output=True, timeout=25)
        return result.stdout.decode("utf-8", errors="replace")
    except Exception:
        return ""

def _curl_head_location(url):
    """curl HEAD è·Ÿéšé‡å®šå‘ï¼Œè¿”å›æœ€ç»ˆ URLï¼ˆç”¨äº /click/ è·³è½¬ï¼‰"""
    try:
        result = subprocess.run(
            ["curl", "-sI", "-L", "--max-time", "10",
             "-H", "User-Agent: Mozilla/5.0 Chrome/120.0.0.0", url],
            capture_output=True, timeout=15)
        text = result.stdout.decode("utf-8", errors="replace")
        locations = re.findall(r'^Location:\s*(\S+)', text, re.IGNORECASE | re.MULTILINE)
        if locations:
            last = locations[-1]
            if last.startswith('http'):
                return last
    except Exception:
        pass
    return url

# â”€â”€ èŒä½è¯¦æƒ…é¡µæŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_job_json(page):
    """ä»é¡µé¢æå– var job = {...} JSONï¼ˆjobs.ac.uk ä¸“ç”¨ï¼‰"""
    m = re.search(r'var\s+job\s*=\s*(\{.*?\});\s*\n', page, re.DOTALL)
    if not m:
        return {}
    try:
        data = json.loads(m.group(1))
        return data.get("job", data)
    except Exception:
        return {}

def scrape_detail(url):
    """è¿”å› (closing_date, apply_url, posted_date, inst)
    - jobs.ac.uk : var job JSON â†’ closing / apply / go_live_dateï¼›inst=""
    - THE Jobs   : JSON-LD validThrough â†’ closingï¼›applicationUrl â†’ applyï¼›inst=""
    - ReliefWeb  : è¯¦æƒ…é¡µæå–æœºæ„åå’Œæˆªæ­¢æ—¥æœŸï¼›apply_url ç›´æ¥ç”¨ reliefweb.int é¡µé¢
    """
    time.sleep(random.uniform(0.3, 1.2))
    try:
        page = _curl_get(url)
        if not page:
            return "", url, "", ""

        is_the = "timeshighereducation.com" in url
        is_rw  = "reliefweb.int"           in url
        closing, apply_url, posted_date, inst = "", url, "", ""

        # â•â• ReliefWeb â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if is_rw:
            # ç”³è¯·é“¾æ¥ï¼šç›´æ¥ç”¨ reliefweb.int é¡µé¢ï¼ˆå«ç”³è¯·ä¿¡æ¯ï¼‰
            apply_url = url

            # æœºæ„åï¼šä¼˜å…ˆä» JSON-LD hiringOrganization
            for blk in re.findall(
                    r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>',
                    page, re.DOTALL):
                try:
                    d = json.loads(blk.strip())
                    org = d.get('hiringOrganization', {})
                    if isinstance(org, dict) and org.get('name'):
                        inst = org['name'].strip()
                        break
                except Exception:
                    pass

            # æœºæ„åï¼š/organization/ é“¾æ¥ï¼ˆæœ€å¯é ï¼‰
            if not inst:
                org_m = re.search(
                    r'<a[^>]+href=["\']?/organization/[^"\'>\s]+["\']?[^>]*>([^<]+)</a>',
                    page, re.IGNORECASE)
                if org_m:
                    inst = org_m.group(1).strip()

            # æœºæ„åï¼šSource / Organization æ ‡ç­¾
            if not inst:
                src_m = re.search(
                    r'(?:Source|Organization)\s*[:\-]?\s*<[^>]*>([^<]{2,80})</[^>]*>',
                    page, re.IGNORECASE)
                if src_m:
                    inst = _strip_tags(src_m.group(1)).strip()

            # æˆªæ­¢æ—¥æœŸï¼šæ–‡æœ¬æ¨¡å¼
            closing_m = re.search(
                r'[Cc]losing\s+[Dd]ate\s*[:\-]?\s*(' + _DATE_PAT + r')', page, re.IGNORECASE)
            if closing_m:
                closing = closing_m.group(1).strip()
            if not closing:
                cd_m = re.search(
                    r'[Cc]losing\s+[Dd]ate.*?(\d{4}-\d{2}-\d{2})', page, re.DOTALL)
                if cd_m:
                    closing = cd_m.group(1)

            return closing, apply_url, posted_date, inst

        # â•â• THE Jobs â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if is_the:
            # apply URLï¼šä» script å—å– "applicationUrl"ï¼ˆå« \u002F ç¼–ç ï¼‰
            m = re.search(r'"applicationUrl"\s*:\s*"([^"]+)"', page, re.IGNORECASE)
            if m:
                raw_url = m.group(1)
                try:
                    decoded = json.loads(f'"{raw_url}"')
                except Exception:
                    decoded = raw_url.replace('\\u002F', '/').replace('\\/', '/')
                if decoded.startswith('http') and 'timeshighereducation' not in decoded:
                    apply_url = decoded

            # closingï¼šJSON-LD validThroughï¼ˆ"validThrough": "2026-03-15T..."ï¼‰
            for blk in re.findall(
                    r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>',
                    page, re.DOTALL):
                try:
                    d = json.loads(blk.strip())
                    vt = d.get('validThrough', '')
                    if vt:
                        closing = vt[:10]   # YYYY-MM-DD
                        break
                except Exception:
                    pass

            # closingï¼š<dt>Closing date</dt><dd>...</dd> HTML ç»“æ„
            if not closing:
                m2 = re.search(
                    r'<dt[^>]*>\s*Closing date\s*</dt>\s*<dd[^>]*>(.*?)</dd>',
                    page, re.IGNORECASE | re.DOTALL)
                if m2:
                    closing = _strip_tags(m2.group(1))

            # closingï¼šæ–‡æœ¬æ¨¡å¼å…œåº•
            if not closing:
                for pat in [
                    r'[Aa]pplication\s+[Dd]eadline\s*[:\-]?\s*(' + _DATE_PAT + r')',
                    r'[Cc]losing\s+[Dd]ate\s*[:\-]?\s*(' + _DATE_PAT + r')',
                    r'[Dd]eadline\s*[:\-]?\s*('         + _DATE_PAT + r')',
                    r'[Aa]pply\s+by\s*[:\-]?\s*('       + _DATE_PAT + r')',
                ]:
                    mc = re.search(pat, page, re.IGNORECASE)
                    if mc:
                        closing = mc.group(1).strip()
                        break

            return closing, apply_url, posted_date, inst   # inst="" for THE Jobs

        # â•â• jobs.ac.uk â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        job_data = _parse_job_json(page)

        # æˆªæ­¢æ—¥æœŸï¼šä¼˜å…ˆ JSONï¼ˆclosing_date å·²æ˜¯ "29th March 2026" æ ¼å¼ï¼‰
        if job_data:
            for key in ("closing_date", "expiring_date", "date_closing", "date_expire"):
                val = job_data.get(key)
                if val:
                    if isinstance(val, (int, float)):
                        closing = datetime.utcfromtimestamp(val).strftime("%-d %B %Y")
                    else:
                        closing = str(val).strip()
                    break

        # æˆªæ­¢æ—¥æœŸï¼šHTML dt/dd å¤‡ç”¨
        if not closing:
            m = re.search(
                r'<dt[^>]*>\s*Closing\s+[Dd]ate\s*</dt>\s*<dd[^>]*>(.*?)</dd>',
                page, re.IGNORECASE | re.DOTALL)
            if m:
                closing = _strip_tags(m.group(1))
        if not closing:
            m = re.search(rf'Closing\s+Date\s*[:\-]\s*({_DATE_PAT})', page, re.IGNORECASE)
            if m:
                closing = m.group(1).strip()
        if not closing:
            m = re.search(rf'closing.{{0,300}}?({_DATE_PAT})', page, re.IGNORECASE | re.DOTALL)
            if m:
                closing = m.group(1).strip()
        if not closing:
            m = re.search(rf'Expir(?:es|y|ation)\s*[:\-]?\s*({_DATE_PAT})', page, re.IGNORECASE)
            if m:
                closing = m.group(1).strip()

        # å‘å¸ƒæ—¥æœŸï¼šgo_live_dateï¼ˆ"20th February 2026"ï¼‰â†’ YYYY-MM-DD
        if job_data:
            gl = job_data.get("go_live_date", "")
            if gl:
                posted_date = _parse_go_live(str(gl))
            if not posted_date:
                dp = job_data.get("date_publish")
                if isinstance(dp, (int, float)) and dp:
                    posted_date = datetime.utcfromtimestamp(dp).strftime("%Y-%m-%d")

        # ç”³è¯·é“¾æ¥ï¼šJSON apply_url â†’ å¤–é“¾ â†’ /click/ è·³è½¬ â†’ /apply/ è·¯å¾„
        if job_data:
            val = job_data.get("apply_url")
            if val and str(val).startswith("http") and "jobs.ac.uk" not in str(val):
                apply_url = str(val)

        if apply_url == url:
            for m2 in re.finditer(
                    r'<a[^>]+href=["\']?(https?://[^"\'>\s]+)["\']?[^>]*>(.*?)</a>',
                    page, re.IGNORECASE | re.DOTALL):
                href      = m2.group(1)
                link_text = _strip_tags(m2.group(2))
                if 'jobs.ac.uk' in href:
                    continue
                if re.search(r'\bapply\b', link_text + ' ' + href, re.IGNORECASE):
                    apply_url = href
                    break

        # /click/ è·Ÿéšé‡å®šå‘
        if apply_url == url:
            m3 = re.search(
                r'href=["\']?(https?://(?:www\.)?jobs\.ac\.uk/job/[^"\'>\s]+/click/[^"\'>\s]*)',
                page, re.IGNORECASE)
            if m3:
                click_url = m3.group(1)
                final = _curl_head_location(click_url)
                apply_url = final if (final and 'jobs.ac.uk' not in final) else click_url

        if apply_url == url:
            m4 = re.search(r'href=["\']?(/job/[^"\'>\s]+/apply/?[^"\'>\s]*)', page, re.IGNORECASE)
            if m4:
                apply_url = BASE + m4.group(1)

        return closing, apply_url, posted_date, ""   # inst="" for jobs.ac.uk

    except Exception:
        return "", url, "", ""


# â”€â”€ jobs.ac.uk RSS æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_rss(subject, path):
    url = BASE + path
    try:
        req = urllib.request.Request(url, headers=RSS_HEADERS)
        with urllib.request.urlopen(req, timeout=20) as r:
            content = r.read()
        content = _fix_entities(content)
        root  = ET.fromstring(content)
        items = root.findall(".//item")
        print(f"  [{subject}] {len(items)} æ¡")
        return items
    except Exception as e:
        print(f"  [{subject}] å¤±è´¥: {e}")
        return []


# â”€â”€ THE Jobs æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _the_classify(title, desc):
    """å…³é”®è¯æ˜ å°„å­¦ç§‘ï¼›æ— åŒ¹é…è¿”å› None"""
    text = (title + " " + desc).lower()
    for keyword, subject in THE_KEYWORD_MAP:
        if keyword in text:
            return subject
    return None

def fetch_the_jobs(seen):
    """ä» THE Jobs å¤šä¸ªå…³é”®è¯ RSS æŠ“å–èŒä½ï¼Œç”¨ pubDate è¿‡æ»¤æœ€è¿‘ THE_DAYS å¤©"""
    from datetime import timezone as _tz
    cutoff     = datetime.now(_tz.utc) - timedelta(days=THE_DAYS)
    seen_links = set()
    all_links  = set()
    new_jobs   = []

    for feed_label, url in THE_RSS_FEEDS:
        try:
            result  = subprocess.run(_CURL_BASE + [url], capture_output=True, timeout=25)
            content = _fix_entities(result.stdout)
            root    = ET.fromstring(content)
            items   = root.findall(".//item")
        except Exception as e:
            print(f"  [THE/{feed_label}] å¤±è´¥: {e}")
            continue

        new_in_feed = 0
        for item in items:
            raw_link = (item.findtext("link") or "").strip()
            if not raw_link:
                continue
            link = re.sub(r'\?.*$', '', raw_link).rstrip('/') + '/'
            all_links.add(link)

            if link in seen_links or link in seen:
                continue
            seen_links.add(link)

            pub_dt = _parse_pubdate(item.findtext("pubDate", ""))
            if not RESET_ALL:
                if pub_dt and pub_dt < cutoff:
                    continue

            pub_date_str = pub_dt.astimezone(SGT).strftime("%Y-%m-%d") if pub_dt else TODAY

            title_raw = (item.findtext("title") or "").strip()
            desc_raw  = html.unescape(item.findtext("description") or "")
            desc_text = re.sub(r'<[^>]+>', ' ', desc_raw)
            desc_text = re.sub(r'\s+', ' ', desc_text).strip()

            subject = _the_classify(title_raw, desc_text)
            if not subject:
                continue   # ä¸åŒ¹é…å­¦ç§‘ä½“ç³»ï¼Œè·³è¿‡

            if ':' in title_raw:
                institution, job_title = title_raw.split(':', 1)
                institution, job_title = institution.strip(), job_title.strip()
            else:
                institution, job_title = "", title_raw

            sal_m = re.search(
                r'(\$[\d,.]+|Â£[\d,.]+|â‚¬[\d,.]+|[\d,.]+\s*(?:USD|GBP|EUR|AUD|CAD)'
                r'|Competitive|Not\s+Specified)',
                desc_text[:300], re.IGNORECASE)
            salary = sal_m.group(1).strip() if sal_m else ""

            new_jobs.append({
                "source":  "THE Jobs",
                "subject": subject,
                "date":    pub_date_str,
                "inst":    institution,
                "title":   job_title,
                "salary":  salary,
                "link":    link,
                "closing": "",
                "apply":   link,
            })
            new_in_feed += 1

        print(f"  [THE/{feed_label}] {len(items)} æ¡RSS â†’ {new_in_feed} æ¡æ–°")

    print(f"  [THE Jobs] åˆè®¡ {len(new_jobs)} æ¡æ–°èŒä½ï¼ˆ{THE_DAYS}å¤©å†…ï¼Œå·²å»é‡ï¼‰")
    return new_jobs, all_links


# â”€â”€ ReliefWeb RSS æŠ“å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_reliefweb_rss(seen, all_links):
    """ä» ReliefWeb RSS æŠ“å–å›½é™…æœºæ„èŒä½ï¼›apply URL å¾… enrich_with_details è¡¥å……"""
    results   = []
    seen_here = seen | all_links

    for label, url in RW_RSS_FEEDS:
        try:
            req = urllib.request.Request(url, headers=RSS_HEADERS)
            with urllib.request.urlopen(req, timeout=20) as r:
                content = r.read()
            content = _fix_entities(content)
            root  = ET.fromstring(content)
            items = root.findall(".//item")
            added = 0
            for item in items:
                link = (item.findtext("link") or "").strip()
                if not link or link in seen_here:
                    continue
                seen_here.add(link)

                pub_raw  = item.findtext("pubDate", "")
                pub_dt   = _parse_pubdate(pub_raw)
                job_date = pub_dt.astimezone(SGT).strftime("%Y-%m-%d") if pub_dt else TODAY

                title_raw = (item.findtext("title") or "").strip()
                desc_raw  = html.unescape(item.findtext("description") or "")
                desc_text = _strip_tags(desc_raw)

                # æœºæ„åï¼šReliefWeb æ ‡é¢˜å¸¸è§æ ¼å¼
                #   "Job Title | Institution"  æˆ–  "Job Title - Institution"
                inst  = ""
                title = title_raw
                if " | " in title_raw:
                    parts = title_raw.split(" | ", 1)
                    title, inst = parts[0].strip(), parts[1].strip()
                elif " - " in title_raw:
                    parts = title_raw.rsplit(" - ", 1)
                    title, inst = parts[0].strip(), parts[1].strip()

                # æˆªæ­¢æ—¥æœŸï¼šå…ˆä» RSS description å°è¯•
                closing = ""
                cd_m = re.search(
                    r'[Cc]losing\s+[Dd]ate\s*[:\-]?\s*(' + _DATE_PAT + r')',
                    desc_text, re.IGNORECASE)
                if cd_m:
                    closing = cd_m.group(1).strip()

                results.append({
                    "source":  "ReliefWeb",
                    "date":    job_date,
                    "inst":    inst,
                    "title":   title,
                    "salary":  "",      # å›½é™…æœºæ„èŒä½è–ªèµ„ä¸æ ‡å‡†ï¼Œç•™ç©º
                    "link":    link,    # reliefweb.int é¡µé¢ï¼Œenrich æ—¶ä¼šæ›¿æ¢ä¸ºåŸå§‹é“¾æ¥
                    "closing": closing,
                    "apply":   link,    # åŒä¸Šï¼Œå¾… enrich æ›¿æ¢
                })
                added += 1

            print(f"  [ReliefWeb/{label}] {len(items)} æ¡RSS â†’ {added} æ¡æ–°")
        except Exception as e:
            print(f"  [ReliefWeb/{label}] å¤±è´¥: {e}")

    return results


# â”€â”€ ä¸»æŠ“å–æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_all(seen):
    jobs_by_subject = {s: [] for s in TARGET_SUBJECTS}
    all_links = set()

    # 1. jobs.ac.uk
    print("\n--- jobs.ac.uk ---")
    if THE_ONLY:
        print("  (è·³è¿‡ï¼Œ--the-only æ¨¡å¼)")
    else:
        for subject, path in SUBJECT_FEEDS:
            items = fetch_rss(subject, path)
            # --week æ¨¡å¼ï¼šæ¯å­¦ç§‘åªå–å‰5æ¡ï¼ŒåŠ é€Ÿæœ¬åœ°éªŒè¯
            if WEEK_MODE:
                items = items[:2]
            for item in items:
                link = (item.findtext("link") or "").strip()
                if not link:
                    continue
                all_links.add(link)
                if link in seen:
                    continue
                title    = (item.findtext("title") or "").strip()
                desc_raw = (item.findtext("description") or "").strip()
                institution, salary = parse_rss_description(desc_raw)
                jobs_by_subject[subject].append({
                    "source":  "jobs.ac.uk",
                    "date":    TODAY,
                    "inst":    institution,
                    "title":   title,
                    "salary":  salary,
                    "link":    link,
                    "closing": "",
                    "apply":   link,
                })

    # 2. THE Jobs
    print("\n--- THE Jobs ---")
    the_jobs, the_links = fetch_the_jobs(seen)
    all_links |= the_links
    for j in the_jobs:
        jobs_by_subject[j["subject"]].append(j)

    # 3. ReliefWeb RSS
    print("\n--- ReliefWeb ---")
    rw_jobs = fetch_reliefweb_rss(seen, all_links)
    for j in rw_jobs:
        jobs_by_subject["International_Orgs"].append(j)
        all_links.add(j["link"])
    if rw_jobs:
        print(f"  [ReliefWeb] åˆè®¡ {len(rw_jobs)} æ¡")

    return jobs_by_subject, all_links


# â”€â”€ è¡¥å……è¯¦æƒ…ï¼ˆå¹¶å‘ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def enrich_with_details(jobs_by_subject):
    """å¹¶å‘æŠ“å–è¯¦æƒ…é¡µï¼Œè¡¥å……æˆªæ­¢æ—¥æœŸã€ç”³è¯·é“¾æ¥ã€å‘å¸ƒæ—¥æœŸï¼ˆjobs.ac.ukï¼‰ã€æœºæ„åï¼ˆReliefWebï¼‰
    scrape_detail è¿”å› (closing, apply_url, posted_date, inst)
    - jobs.ac.uk : JSON â†’ closing / apply / go_live_dateï¼›inst å¿½ç•¥
    - THE Jobs   : JSON-LD validThrough / applicationUrlï¼›inst å¿½ç•¥
    - ReliefWeb  : æœºæ„åä»è¯¦æƒ…é¡µæå–ï¼›apply_url = reliefweb.int é¡µé¢
    """
    all_jobs = [j for subj in TARGET_SUBJECTS for j in jobs_by_subject[subj]
                if j["source"] in ("jobs.ac.uk", "THE Jobs", "ReliefWeb")]
    total = len(all_jobs)
    if total == 0:
        return

    print(f"\næŠ“å– {total} ä¸ªèŒä½è¯¦æƒ…é¡µï¼ˆå¹¶å‘ 5 çº¿ç¨‹ï¼Œå«éšæœºå»¶è¿Ÿï¼‰...")
    done = 0
    with ThreadPoolExecutor(max_workers=5) as ex:
        f_map = {ex.submit(scrape_detail, j["link"]): j for j in all_jobs}
        for f in as_completed(f_map):
            j = f_map[f]
            closing, apply_url, posted_date, inst = f.result()
            if closing:
                j["closing"] = closing
            j["apply"] = apply_url
            if posted_date:          # jobs.ac.uk çœŸå®å‘å¸ƒæ—¥æœŸ
                j["date"] = posted_date
            if inst and j["source"] == "ReliefWeb":   # ReliefWeb æœºæ„å
                j["inst"] = inst
            done += 1
            if done % 20 == 0 or done == total:
                print(f"  {done}/{total} å®Œæˆ")


# â”€â”€ å†™å…¥ Google Sheets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_to_sheets(jobs_by_subject):
    rows = []
    for subj in TARGET_SUBJECTS:
        display_subj = "" if subj == "International_Orgs" else subj
        for j in jobs_by_subject[subj]:
            rows.append([
                "'" + j["date"],   # åŠ  ' é˜²æ­¢ Sheets æŠŠæ—¥æœŸè§£ææˆå…¶ä»–æ ¼å¼
                display_subj,
                j["inst"],
                j["title"],
                j["salary"],
                j["closing"],
                j["apply"],
                j.get("source", ""),
            ])

    if not rows:
        print("æ²¡æœ‰æ–°èŒä½")
        return False

    try:
        import base64
        cred_json = os.environ.get("GOOGLE_SERVICE_ACCOUNT", "")
        if cred_json:
            try:
                sa_info = json.loads(base64.b64decode(cred_json))
            except Exception:
                sa_info = json.loads(cred_json)
            creds = Credentials.from_service_account_info(
                sa_info,
                scopes=["https://www.googleapis.com/auth/spreadsheets",
                        "https://www.googleapis.com/auth/drive"])
        else:
            import google.auth
            creds, _ = google.auth.default(
                scopes=["https://www.googleapis.com/auth/spreadsheets",
                        "https://www.googleapis.com/auth/drive"])
        gc = gspread.authorize(creds)
        ws = gc.open_by_key(SHEET_ID).worksheet(SHEET_RANGE)
        # æ—¶é—´æˆ³è¡Œï¼ˆç½®é¡¶ï¼‰+ æ•°æ® + ç©ºè¡Œåˆ†éš”
        ts            = datetime.now(SGT).strftime("%Y/%m/%d, %H:%M") + "å®Œæˆæ›´æ–°"
        timestamp_row = [[ts] + [""] * (len(rows[0]) - 1)]
        separator     = [[""] * len(rows[0])]
        ws.insert_rows(timestamp_row + rows + separator, row=2,
                       value_input_option="USER_ENTERED")
        print(f"âœ“ æˆåŠŸå†™å…¥ {len(rows)} æ¡ï¼ˆå·²ç½®é¡¶ï¼‰")
        return True
    except Exception as e:
        print(f"Sheets å†™å…¥å¼‚å¸¸: {e}")
        return False


# â”€â”€ ä¸»å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    mode = "å…¨é‡æ¨¡å¼ï¼ˆ--allï¼‰" if RESET_ALL else ("é™é€Ÿæ¨¡å¼ï¼ˆ--weekï¼‰" if WEEK_MODE else "å¢é‡æ¨¡å¼")
    print(f"=== æŠ“å–å­¦æœ¯èŒä½ [jobs.ac.uk + THE Jobs + ReliefWeb] [{mode}] ===")
    print(f"ğŸ“… æŠ“å–èŒƒå›´: {DATE_LABEL}")

    seen = load_seen()
    print(f"å·²è®°å½• {len(seen)} æ¡å†å²èŒä½")

    jobs, all_links = fetch_all(seen)
    total_new = sum(len(v) for v in jobs.values())

    print(f"\nå‘ç° {total_new} æ¡æ–°èŒä½")
    for subj in TARGET_SUBJECTS:
        if jobs[subj]:
            print(f"  {subj}: {len(jobs[subj])}")

    if total_new:
        enrich_with_details(jobs)
        ok = write_to_sheets(jobs)
        if ok:
            save_seen(seen | all_links)
            print(f"å·²æ›´æ–°è®°å½•ï¼ˆå…± {len(seen | all_links)} æ¡ï¼‰")
    else:
        save_seen(seen | all_links)
